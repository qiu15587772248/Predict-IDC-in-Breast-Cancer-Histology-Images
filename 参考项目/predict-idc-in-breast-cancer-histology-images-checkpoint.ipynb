{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "570c24f2-126a-4fd3-bae2-e8247646f36d",
    "_uuid": "118c9d31c593676aaf77c8975339e52d7cb65cdf"
   },
   "source": [
    "**Predicting IDC in Breast Cancer Histology Images** \n",
    "* Part One: https://www.kaggle.com/paultimothymooney/predicting-idc-in-breast-cancer-histology-images/\n",
    "* * Model Selection (see link above)\n",
    "*  Part Two: Predict IDC in Breast Cancer\n",
    "* * Model Evaluation (see below)\n",
    "\n",
    "Breast cancer is the most common form of cancer in women, and invasive ductal carcinoma (IDC) is the most common form of breast cancer. Accurately identifying and categorizing breast cancer subtypes is an important clinical task, and automated methods can be used to save time and reduce error.\n",
    "\n",
    "The goal of this script is to identify IDC when it is present in otherwise unlabeled histopathology images. The dataset consists of 277,524 50x50 pixel RGB digital image patches that were derived from 162 H&E-stained breast histopathology samples. These images are small patches that were extracted from digital images of breast tissue samples. The breast tissue contains many cells but only some of them are cancerous. Patches that are labeled \"1\" contain cells that are characteristic of invasive ductal carcinoma. For more information about the data, see https://www.ncbi.nlm.nih.gov/pubmed/27563488 and http://spie.org/Publications/Proceedings/Paper/10.1117/12.2043872."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb849bd9-7894-4fd3-b55d-4d0a77c9161c",
    "_uuid": "debe60106fe79d3b2316308fd11f3d2399255c61"
   },
   "source": [
    "*Step 1: Import Modules*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "bb9fe3ef-104a-4a98-9f87-b65906acf50f",
    "_kg_hide-input": true,
    "_uuid": "5a9827f8d703a8741175d1e7df3121632715bdd8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "import itertools\n",
    "import fnmatch\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "# from scipy.misc import imresize, imread # 这两个函数在新版scipy中已移除，后续会用cv2或PIL替代\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# Keras 相关导入被 PyTorch 替代\n",
    "\n",
    "# --- PyTorch 相关导入 ---\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image # 用于图像读取\n",
    "# --------------------------\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c6bd2df-2a43-4604-b407-ba81c8c3939b",
    "_uuid": "540f3c39324580297bcff94b4c4cfc0c5bb45b5f"
   },
   "source": [
    "*Step 2: Explore Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "765908c1-86cc-41df-8479-a39174231237",
    "_uuid": "1978266e1e2fe5d6eb9bdc17deb0d309441065a2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 定义包含数据集的本地基础路径\n",
    "# 注意：在Windows上，路径中的反斜杠 \\ 需要转义为 \\\\，或者直接使用正斜杠 /\n",
    "# 使用原始字符串 r\"...\" 可以避免对反斜杠进行转义的需要\n",
    "base_data_dir = r\"E:\\datasets\\IDC_data\"\n",
    "\n",
    "# 构建glob模式以匹配所有PNG文件\n",
    "# 模式解释：\n",
    "# base_data_dir      -> E:\\datasets\\IDC_data\n",
    "# /**                -> 匹配任何直接子文件夹（即患者ID文件夹，如8863, 8864等）\n",
    "# /**                -> 匹配这些患者ID文件夹下的任何子文件夹（即0或1文件夹）\n",
    "# /*.png             -> 匹配这些0或1文件夹中所有以 .png 结尾的文件\n",
    "\n",
    "# os.path.join 会帮助处理路径分隔符，使其跨平台兼容\n",
    "# 对于 E:\\datasets\\IDC_data\\PATIENT_ID\\CLASS_ID\\image.png 这样的结构：\n",
    "# image_path_pattern = os.path.join(base_data_dir, '*', '*', '*.png')\n",
    "# 然而，使用 recursive=True 和 '**' 通常更灵活，可以处理不同深度的嵌套\n",
    "image_path_pattern = os.path.join(base_data_dir, '**', '*.png')\n",
    "\n",
    "\n",
    "print(f\"正在使用的glob模式: {image_path_pattern}\")\n",
    "\n",
    "# 使用glob获取所有匹配的图像文件路径\n",
    "# recursive=True 允许 '**' 匹配任意层级的目录\n",
    "imagePatches = glob(image_path_pattern, recursive=True)\n",
    "\n",
    "# 过滤掉非文件（例如目录自身如果被glob不小心匹配到，虽然用'*.png'不太可能）\n",
    "imagePatches = [path for path in imagePatches if os.path.isfile(path)]\n",
    "\n",
    "\n",
    "# 打印找到的图像数量和前10个文件的路径以供验证\n",
    "print(f\"总共找到 {len(imagePatches)} 张图像补丁。\")\n",
    "if len(imagePatches) > 0:\n",
    "    print(\"\\n前10张图像路径示例:\")\n",
    "    for filename in imagePatches[0:10]:\n",
    "        print(filename)\n",
    "    # 验证一下最后10个，确保glob正常工作\n",
    "    print(\"\\n后10张图像路径示例 (如果总数大于10):\")\n",
    "    for filename in imagePatches[-10:]:\n",
    "        print(filename)\n",
    "\n",
    "else:\n",
    "    print(\"\\n警告: 未找到任何图像文件。请仔细检查 base_data_dir ('E:\\\\datasets\\\\IDC_data') 和 glob模式。\")\n",
    "    print(\"请确保 E:\\\\datasets\\\\IDC_data 文件夹下确实存在如 8863\\\\0\\\\*.png 这样的文件。\")\n",
    "\n",
    "# 检查是否有重复的路径（一般不太可能，但可以作为健全性检查）\n",
    "if len(imagePatches) > 0 and len(imagePatches) != len(set(imagePatches)):\n",
    "    print(\"\\n警告: imagePatches 列表中存在重复路径！这可能意味着glob模式有问题。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "0fd6c75a-b439-455c-ba85-24aa9d83da5b",
    "_uuid": "1d105b2c6fff7ddfd3120215ea7f77723e360179",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_name = \"/kaggle/input/IDC_regular_ps50_idx5/9135/1/9135_idx5_x1701_y1851_class1.png\" #Image to be used as query\n",
    "def plotImage(image_location):\n",
    "    image = cv2.imread(image_name)\n",
    "    image = cv2.resize(image, (50,50))\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
    "    return\n",
    "plotImage(image_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "cfc417f1-c12a-42df-b264-c4e7e6e79da8",
    "_uuid": "f9d9578ed454625dbe08a3185865cc3ae556b993",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot Multiple Images\n",
    "bunchOfImages = imagePatches\n",
    "i_ = 0\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "for l in bunchOfImages[:25]:\n",
    "    im = cv2.imread(l)\n",
    "    im = cv2.resize(im, (50, 50)) \n",
    "    plt.subplot(5, 5, i_+1) #.set_title(l)\n",
    "    plt.imshow(cv2.cvtColor(im, cv2.COLOR_BGR2RGB)); plt.axis('off')\n",
    "    i_ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "f010fda5-40f0-47ce-89c0-40d21a754f59",
    "_uuid": "10cc771383f08fd404419bccaa00ed9ad99cd0e2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def randomImages(a):\n",
    "    r = random.sample(a, 4)\n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(cv2.imread(r[0]))\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(cv2.imread(r[1]))\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(cv2.imread(r[2])); \n",
    "randomImages(imagePatches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5f695df0-da48-4cc5-93a0-dcb7a82d21a3",
    "_uuid": "c88fc872e6af3918192df09635e88904145761a2"
   },
   "source": [
    "*Step 3: Preprocess Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "939bc134-6122-4d4d-aa6c-89a47318896d",
    "_uuid": "aae6a2ec2ab20bd94422e29b147a45a880fb480a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "patternZero = '*class0.png'\n",
    "patternOne = '*class1.png'\n",
    "classZero = fnmatch.filter(imagePatches, patternZero)\n",
    "classOne = fnmatch.filter(imagePatches, patternOne)\n",
    "print(\"IDC(-)\\n\\n\",classZero[0:5],'\\n')\n",
    "print(\"IDC(+)\\n\\n\",classOne[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IDCDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (list): 包含所有图像文件路径的列表。\n",
    "            transform (callable, optional): 应用于样本的可选转换。\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "        # 从文件名确定标签：0 代表 class0 (IDC-), 1 代表 class1 (IDC+)\n",
    "        # 确保 image_paths 中的路径字符串与您的文件名模式匹配\n",
    "        self.labels = [1 if 'class1.png' in img_path.lower() else 0 for img_path in self.image_paths]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_path = self.image_paths[idx]\n",
    "        # 使用 Pillow (PIL) 读取图像，并确保它是RGB格式\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            print(f\"错误: 文件未找到 {img_path}\")\n",
    "            # 您可以在这里返回一个占位符图像和标签，或者引发异常\n",
    "            # 为简单起见，我们暂时跳过这个样本（这在实际中可能需要更鲁棒的处理）\n",
    "            # 返回 None 会在 DataLoader 中被 collate_fn 过滤掉，或者您可以自定义 collate_fn\n",
    "            return None, None \n",
    "        except Exception as e:\n",
    "            print(f\"读取图像时发生错误 {img_path}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# --- 图像转换 ---\n",
    "# 1. 将图像大小调整为 50x50 (与原notebook一致)\n",
    "# 2. 将 PIL.Image 对象转换为 PyTorch Tensor (并将像素值从 [0, 255] 缩放到 [0.0, 1.0])\n",
    "# 3. 对图像进行标准化 (可选，但推荐)\n",
    "#    原notebook中是 X = X/255.0，ToTensor() 已经完成了这个范围的缩放。\n",
    "#    更进一步的标准化可以使用 transforms.Normalize。\n",
    "#    您可以根据您的数据特性计算均值和标准差，或使用ImageNet的预训练值。\n",
    "preprocess_transform = transforms.Compose([\n",
    "    transforms.Resize((50, 50)),\n",
    "    transforms.ToTensor(),\n",
    "    # 例如，使用ImageNet的均值和标准差进行标准化\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    # 或者，如果您在后续步骤计算了自己数据集的均值和标准差，可以在这里使用\n",
    "    # transforms.Normalize(mean=[YOUR_MEAN_R, YOUR_MEAN_G, YOUR_MEAN_B], std=[YOUR_STD_R, YOUR_STD_G, YOUR_STD_B])\n",
    "])\n",
    "\n",
    "print(\"IDCDataset 类和 preprocess_transform 已定义。\")\n",
    "print(\"请注意：上面的代码块只是定义了类和转换。\")\n",
    "print(\"您需要在后续的单元格中实例化 IDCDataset 并创建 DataLoader。\")\n",
    "print(\"例如:\")\n",
    "print(\"# 假设 imagePatches 变量已通过 glob 获取了所有图像路径\")\n",
    "print(\"# (确保在运行此代码前，包含 `imagePatches = glob(...)` 的单元格已运行)\")\n",
    "print(\"# 并且 imagePatches 中的路径是您本地环境可访问的正确路径。\")\n",
    "print(\"# full_dataset = IDCDataset(image_paths=imagePatches, transform=preprocess_transform)\")\n",
    "print(\"# print(f'完整数据集大小: {len(full_dataset)}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "f62e9e56-6564-4376-845f-284f624fa932",
    "_uuid": "69640054f59133ebe8dfa06612f3c14383916e90",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def proc_images(lowerIndex,upperIndex):\n",
    "    \"\"\"\n",
    "    Returns two arrays: \n",
    "        x is an array of resized images\n",
    "        y is an array of labels\n",
    "    \"\"\" \n",
    "    x = []\n",
    "    y = []\n",
    "    WIDTH = 50\n",
    "    HEIGHT = 50\n",
    "    for img in imagePatches[lowerIndex:upperIndex]:\n",
    "        full_size_image = cv2.imread(img)\n",
    "        x.append(cv2.resize(full_size_image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC))\n",
    "        if img in classZero:\n",
    "            y.append(0)\n",
    "        elif img in classOne:\n",
    "            y.append(1)\n",
    "        else:\n",
    "            return\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "3bf67ef8-5613-4862-bc46-0106d1880981",
    "_uuid": "2b8fd5414423f1e7ff7da620d8672bb496a9ef57",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X,Y = proc_images(0,90000)\n",
    "df = pd.DataFrame()\n",
    "df[\"images\"]=X\n",
    "df[\"labels\"]=Y\n",
    "X2=df[\"images\"]\n",
    "Y2=df[\"labels\"]\n",
    "X2=np.array(X2)\n",
    "imgs0=[]\n",
    "imgs1=[]\n",
    "imgs0 = X2[Y2==0] # (0 = no IDC, 1 = IDC)\n",
    "imgs1 = X2[Y2==1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b3c1681f-7576-4b3d-ad49-7832e3bcf209",
    "_uuid": "5660e633007abaeda0a4698b8bfacf55d9552ab4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def describeData(a,b):\n",
    "    print('Total number of images: {}'.format(len(a)))\n",
    "    print('Number of IDC(-) Images: {}'.format(np.sum(b==0)))\n",
    "    print('Number of IDC(+) Images: {}'.format(np.sum(b==1)))\n",
    "    print('Percentage of positive images: {:.2f}%'.format(100*np.mean(b)))\n",
    "    print('Image shape (Width, Height, Channels): {}'.format(a[0].shape))\n",
    "describeData(X2,Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "0c3b967b-674c-4efa-b59e-56772166ef41",
    "_uuid": "9eed051f46211121a7add96c9cf86165340a7e28",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dict_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "print(df.head(10))\n",
    "print(\"\")\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "d72d8a7d-7234-4a68-92f9-8f3578e08e36",
    "_uuid": "3836924a638d95d90d85a02829bd85933f3c495f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plotOne(a,b):\n",
    "    \"\"\"\n",
    "    Plot one numpy array\n",
    "    \"\"\"\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('IDC (-)')\n",
    "    plt.imshow(a[0])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('IDC (+)')\n",
    "    plt.imshow(b[0])\n",
    "plotOne(imgs0, imgs1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "389ea274-9f97-45e6-a99f-d3362412e721",
    "_uuid": "c83c02425f516da672e7957f0cd42ecc72dce981",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plotTwo(a,b): \n",
    "    \"\"\"\n",
    "    Plot a bunch of numpy arrays sorted by label\n",
    "    \"\"\"\n",
    "    for row in range(3):\n",
    "        plt.figure(figsize=(20, 10))\n",
    "        for col in range(3):\n",
    "            plt.subplot(1,8,col+1)\n",
    "            plt.title('IDC (-)')\n",
    "            plt.imshow(a[0+row+col])\n",
    "            plt.axis('off')       \n",
    "            plt.subplot(1,8,col+4)\n",
    "            plt.title('IDC (+)')\n",
    "            plt.imshow(b[0+row+col])\n",
    "            plt.axis('off')\n",
    "plotTwo(imgs0, imgs1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "254e9903-c303-46cd-bde9-0d3b412e6cdb",
    "_uuid": "4d584d18cb934f1cf025e9bbb6d6cb8c03542494"
   },
   "source": [
    "The data is scaled from 0 to 256 but we want it to be scaled from 0 to 1. This will make the data compatible with a wide variety of different classification algorithms.  We also want to set aside 20% of the data for testing. This will make the trained model less prone to overfitting.  And finally, we will use an oversampling strategy to deal with the imbalanced class sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "c5d01476-b570-4fb4-8ebb-1af69f3e215e",
    "_uuid": "23ca1626461e4749493064070986a075f8317539",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def plotHistogram(a):\n",
    "    \"\"\"\n",
    "    Plot histogram of RGB Pixel Intensities\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(a)\n",
    "    plt.axis('off')\n",
    "    plt.title('IDC(+)' if Y[1] else 'IDC(-)')\n",
    "    histo = plt.subplot(1,2,2)\n",
    "    histo.set_ylabel('Count')\n",
    "    histo.set_xlabel('Pixel Intensity')\n",
    "    n_bins = 30\n",
    "    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n",
    "    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n",
    "    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\n",
    "plotHistogram(X2[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "9f9e4c80-a8e0-47ac-957b-22231b1e9fca",
    "_uuid": "ae82d3f7ec59d7d112a8dd9ed1b8121a4792a1b0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X=np.array(X)\n",
    "X=X/255.0\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "\n",
    "# Reduce Sample Size for DeBugging\n",
    "X_train = X_train[0:300000] \n",
    "Y_train = Y_train[0:300000]\n",
    "X_test = X_test[0:300000] \n",
    "Y_test = Y_test[0:300000]\n",
    "\n",
    "print(\"Training Data Shape:\", X_train.shape)\n",
    "print(\"Testing Data Shape:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "b07de5e9-53dd-49e5-9da3-778bc9eb9b17",
    "_uuid": "5d808c28fb0f9aaa0364d8b62d11fc1f2810c2c8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plotHistogram(X_train[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "a23e5463-5d12-43c8-8a3a-eb7dcfecdf6b",
    "_uuid": "447f1371f137cf51962a0f32171d44a3080a694c",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_trainHot = to_categorical(Y_train, num_classes = 2)\n",
    "Y_testHot = to_categorical(Y_test, num_classes = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "ab055bcf-2a2c-4645-b043-45d28ddcc782",
    "_uuid": "1c05f8d5a5ada31b3741944ac02d480c2b39619f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lab = df['labels']\n",
    "dist = lab.value_counts()\n",
    "sns.countplot(lab)\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "06cb3ea4-371a-4971-9449-dc726f06e16b",
    "_uuid": "5d3087f0dfe874a591449b9d5f27da138f42f0c6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Deal with imbalanced class sizes below\n",
    "# Make Data 1D for compatability upsampling methods\n",
    "X_trainShape = X_train.shape[1]*X_train.shape[2]*X_train.shape[3]\n",
    "X_testShape = X_test.shape[1]*X_test.shape[2]*X_test.shape[3]\n",
    "X_trainFlat = X_train.reshape(X_train.shape[0], X_trainShape)\n",
    "X_testFlat = X_test.reshape(X_test.shape[0], X_testShape)\n",
    "#print(\"X_train Shape: \",X_train.shape)\n",
    "#print(\"X_test Shape: \",X_test.shape)\n",
    "#print(\"X_trainFlat Shape: \",X_trainFlat.shape)\n",
    "#print(\"X_testFlat Shape: \",X_testFlat.shape)\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "#ros = RandomOverSampler(ratio='auto')\n",
    "ros = RandomUnderSampler(ratio='auto')\n",
    "X_trainRos, Y_trainRos = ros.fit_sample(X_trainFlat, Y_train)\n",
    "X_testRos, Y_testRos = ros.fit_sample(X_testFlat, Y_test)\n",
    "\n",
    "# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\n",
    "Y_trainRosHot = to_categorical(Y_trainRos, num_classes = 2)\n",
    "Y_testRosHot = to_categorical(Y_testRos, num_classes = 2)\n",
    "#print(\"X_train: \", X_train.shape)\n",
    "#print(\"X_trainFlat: \", X_trainFlat.shape)\n",
    "#print(\"X_trainRos Shape: \",X_trainRos.shape)\n",
    "#print(\"X_testRos Shape: \",X_testRos.shape)\n",
    "#print(\"Y_trainRosHot Shape: \",Y_trainRosHot.shape)\n",
    "#print(\"Y_testRosHot Shape: \",Y_testRosHot.shape)\n",
    "\n",
    "for i in range(len(X_trainRos)):\n",
    "    height, width, channels = 50,50,3\n",
    "    X_trainRosReshaped = X_trainRos.reshape(len(X_trainRos),height,width,channels)\n",
    "#print(\"X_trainRos Shape: \",X_trainRos.shape)\n",
    "#print(\"X_trainRosReshaped Shape: \",X_trainRosReshaped.shape)\n",
    "\n",
    "for i in range(len(X_testRos)):\n",
    "    height, width, channels = 50,50,3\n",
    "    X_testRosReshaped = X_testRos.reshape(len(X_testRos),height,width,channels)\n",
    "#print(\"X_testRos Shape: \",X_testRos.shape)\n",
    "#print(\"X_testRosReshaped Shape: \",X_testRosReshaped.shape)\n",
    "\n",
    "dfRos = pd.DataFrame()\n",
    "dfRos[\"labels\"]=Y_trainRos\n",
    "labRos = dfRos['labels']\n",
    "distRos = lab.value_counts()\n",
    "sns.countplot(labRos)\n",
    "print(dict_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "11ec5cdf-44ad-4bf6-8975-4fa4682d5e2a",
    "_uuid": "9265cdcdeb26c3169db18392ce1603885129a130"
   },
   "source": [
    "*Step 4: Define Helper Functions for the Classification Task*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "5a0e446b-a55a-4933-b4a7-198da1669545",
    "_uuid": "ec338837016d3d5222e98a10331b1b2de457db34",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\n",
    "print(\"Old Class Weights: \",class_weight)\n",
    "from sklearn.utils import class_weight\n",
    "class_weight2 = class_weight.compute_class_weight('balanced', np.unique(Y_trainRos), Y_trainRos)\n",
    "print(\"New Class Weights: \",class_weight2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "137bc037-a8a4-4dac-aecd-62d6efc6c127",
    "_uuid": "4a883a6aaddd3261a630af0f12ed7e542da3eb3f",
    "collapsed": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Helper Functions  Learning Curves and Confusion Matrix\n",
    "\n",
    "class MetricsCheckpoint(Callback):\n",
    "    \"\"\"Callback that saves metrics after each epoch\"\"\"\n",
    "    def __init__(self, savepath):\n",
    "        super(MetricsCheckpoint, self).__init__()\n",
    "        self.savepath = savepath\n",
    "        self.history = {}\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "        np.save(self.savepath, self.history)\n",
    "\n",
    "def plotKerasLearningCurve():\n",
    "    plt.figure(figsize=(10,5))\n",
    "    metrics = np.load('logs.npy')[()]\n",
    "    filt = ['acc'] # try to add 'loss' to see the loss learning curve\n",
    "    for k in filter(lambda x : np.any([kk in x for kk in filt]), metrics.keys()):\n",
    "        l = np.array(metrics[k])\n",
    "        plt.plot(l, c= 'r' if 'val' not in k else 'b', label='val' if 'val' in k else 'train')\n",
    "        x = np.argmin(l) if 'loss' in k else np.argmax(l)\n",
    "        y = l[x]\n",
    "        plt.scatter(x,y, lw=0, alpha=0.25, s=100, c='r' if 'val' not in k else 'b')\n",
    "        plt.text(x, y, '{} = {:.4f}'.format(x,y), size='15', color= 'r' if 'val' not in k else 'b')   \n",
    "    plt.legend(loc=4)\n",
    "    plt.axis([0, None, None, None]);\n",
    "    plt.grid()\n",
    "    plt.xlabel('Number of epochs')\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = (5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "def plot_learning_curve(history):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./accuracy_curve.png')\n",
    "    #plt.clf()\n",
    "    # summarize history for loss\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig('./loss_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0fe72b01-0848-4ac9-b4b8-a2d212cfcc2d",
    "_uuid": "4524ae34f83914f75a2da05afba664cf7f0ea505"
   },
   "source": [
    "*Step 5: Evaluate Classification Models*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fde814f1-38f0-402f-81da-c7f899791b4b",
    "_uuid": "5fb738a2cb7f08a57ca493c948a44d9df975d3d7"
   },
   "source": [
    "In a previous kernel I evaluated a number of different classification algorithms while using an abbreviated form of this same dataset.  To see how and why I chose the model that I use below, please see the following link: https://www.kaggle.com/paultimothymooney/predicting-idc-in-breast-cancer-histology-images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "47acf66e-5542-4f73-908c-11f76ed8becf",
    "_uuid": "6d7e57d88e5578aea8b0b8f397d2a689e9c74de0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def runKerasCNNAugment(a,b,c,d,e,f):\n",
    "    \"\"\"\n",
    "    Run Keras CNN: https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py\n",
    "    \"\"\"\n",
    "    batch_size = 128\n",
    "    num_classes = 2\n",
    "    epochs = 8\n",
    "#     img_rows, img_cols = a.shape[1],a.shape[2]\n",
    "    img_rows,img_cols=50,50\n",
    "    input_shape = (img_rows, img_cols, 3)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape,strides=e))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=True)  # randomly flip images\n",
    "    history = model.fit_generator(datagen.flow(a,b, batch_size=32),\n",
    "                        steps_per_epoch=len(a) / 32, epochs=epochs,class_weight=f, validation_data = [c, d],callbacks = [MetricsCheckpoint('logs')])\n",
    "    score = model.evaluate(c,d, verbose=0)\n",
    "    print('\\nKeras CNN #1C - accuracy:', score[1],'\\n')\n",
    "    y_pred = model.predict(c)\n",
    "    map_characters = {0: 'IDC(-)', 1: 'IDC(+)'}\n",
    "    print('\\n', sklearn.metrics.classification_report(np.where(d > 0)[1], np.argmax(y_pred, axis=1), target_names=list(map_characters.values())), sep='')    \n",
    "    Y_pred_classes = np.argmax(y_pred,axis=1) \n",
    "    Y_true = np.argmax(d,axis=1) \n",
    "    plotKerasLearningCurve()\n",
    "    plt.show()  \n",
    "    plot_learning_curve(history)\n",
    "    plt.show()\n",
    "    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
    "    plot_confusion_matrix(confusion_mtx, classes = list(dict_characters.values())) \n",
    "    plt.show()\n",
    "runKerasCNNAugment(X_trainRosReshaped, Y_trainRosHot, X_testRosReshaped, Y_testRosHot,2,class_weight2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a03f19fc-05c4-4f8a-8e0b-a921c781ff10",
    "_uuid": "7bc3bc872c444fe5990aa365a832b6dc9c9bdad8"
   },
   "source": [
    "Next I will try one more time but without the undersampling step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "97db66e5-0fc8-4e1d-bfc0-179c3391f08f",
    "_uuid": "9854e5bb4554a4bea7d59a37ec6d4a24ce141fc0",
    "collapsed": true,
    "scrolled": false,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#runKerasCNNAugment(X_train, Y_trainHot, X_test, Y_testHot,2,class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3e4418ff-dabc-46b4-8fa9-7daaab1402a7",
    "_uuid": "069b85a95cbc0c4aa7e71b64ad9ef3c8aebf4373"
   },
   "source": [
    "90+% accuracy is pretty good!  And it does not look too be to overfit or too biased based off of the learning curve and confusion matrix.  In the future, I will improve the score by optimizing the data augmentation step as well as the network architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
